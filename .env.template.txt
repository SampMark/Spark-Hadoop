# O arquivo .env serve para configurar versões (HADOOP_VERSION, SPARK_VERSION), portas, quantidade de nós, 
# entre outros parâmetros. 
# O repositório Spark-Hadoop provê o modelo `.env.template` para que o usuário possa copiar e editar, 
# documentando as opções. A forma de carregar essas variáveis nos contêineres utiliza a diretiva `env_file` 
# do Compose para injetar as variáveis, o que aumenta a clareza e evita expor credenciais no sistema de arquivos do contêiner.
# Edite MY_USERNAME, NUM_WORKER_NODES, HADOOP_VERSION, SPARK_VERSION, 
# JUPYTER_PASSWORD_HASH, etc.

# Usuário criado dentro do container
MY_USERNAME=sparkuser

# Prefixo para Stack, containers e volumes
STACK_NAME=hadoop-spark-cluster

# Número de nós worker (DataNode + NodeManager + Spark Worker)
NUM_WORKER_NODES=2

# Hadoop
HADOOP_VERSION=3.4.2
HADOOP_URL=https://downloads.apache.org/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz
HADOOP_NAMENODE_HOST=master
HDFS_REPLICATION_FACTOR=1
DFS_REPLICATION=1
HDFS_NAMENODE_HOST=namenode

# YARN
YARN_NODEMANAGER_RESOURCE_MEMORY_MB=2048
YARN_SCHEDULER_MAXIMUM_ALLOCATION_MB=1536
YARN_RESOURCEMANAGER_HEAPSIZE_MB=1024
YARN_RESOURCEMANAGER_MAXIMUM_ALLOCATION_MB=1024
# YARN_NM_MEM_MB=4096

# Spark
SPARK_VERSION=4.0.0
SPARK_URL=https://downloads.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.2.tgz
SPARK_MASTER_HOST=spark-master
SPARK_WORKER_CORES=2
SPARK_WORKER_MEMORY=2048M
SPARK_HISTORY_SERVER_ENABLED=true

# JupyterLab
JUPYTER_ENABLE=true
JUPYTER_PASSWORD_HASH=<hash gerado via ipython>  # Execute: python3 -c "from notebook.auth import passwd; print(passwd('sua_senha'))"
JUPYTER_PORT=8888

# Spark Connect Server (habilita modo headless, sem Jupyter)
SPARK_CONNECT_SERVER=false
SPARK_CONNECT_PORT=15002

# Versão do Java (deve ser compatível com Hadoop e Spark)
JAVA_VERSION=11

# SSH
SSH_KEY_TYPE=rsa
SSH_KEY_BITS=2048

# Portas
HOST_HDFS_UI_PORT=9870
HOST_YARN_UI_PORT=8088
HOST_SPARK_UI_PORT=7077
HOST_SPARK_HISTORY_UI_PORT=18080

# Timeout de healthchecks (segundos)
HEALTHCHECK_TIMEOUT=60


# Spark-connect server? [enable/disable]
SPARK_CONNECT_SERVER=disable
# Ao definir SPARK_CONNECT_SERVER=enable, ao subir o cluster:
# O container “master” (Spark Master) passa a expor um endpoint gRPC (na porta configurada em SPARK_CONNECT_PORT),
# para aceitar conexões de clientes externos.
# O kernel PySpark local (no próprio container ou no seu terminal) NÃO funcionará em modo interativo “local” 
# porque, ao habilitar Spark Connect, o ponto de entrada do Spark muda para esse servidor remoto.
# Consequentemente, o JupyterLab que roda no mesmo container deixará de criar um kernel PySpark “in-process” e, 
# em vez disso, qualquer notebook PySpark precisará ser configurado para se conectar via Spark Connect 
# (normalmente fornecendo o endereço gRPC do driver).

# Docker image name
IMAGE_NAME=spark:latest
STACK_NAME=spark

# Ubuntu apt mirror
# Optional: Set a custom Ubuntu APT mirror (e.g., a local or regional mirror)
# Leave this variable blank to use the default mirror (http://archive.ubuntu.com/ubuntu)
APT_MIRROR=http://br.archive.ubuntu.com/ubuntu