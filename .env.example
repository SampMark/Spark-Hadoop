
# Edite MY_USERNAME, NUM_WORKER_NODES, HADOOP_VERSION, SPARK_VERSION, 
# JUPYTER_PASSWORD_HASH, etc.

# Usuário criado dentro do container
MY_USERNAME=sparkuser

# Prefixo para Stack, containers e volumes
STACK_NAME=hadoop-spark-cluster

# Número de nós worker (DataNode + NodeManager + Spark Worker)
NUM_WORKER_NODES=2

# Hadoop
HADOOP_VERSION=3.4.1
HADOOP_URL=https://downloads.apache.org/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz
HDFS_REPLICATION_FACTOR=1
HDFS_NAMENODE_HOST=namenode

# YARN
YARN_NODEMANAGER_RESOURCE_MEMORY_MB=2048
YARN_SCHEDULER_MAXIMUM_ALLOCATION_MB=1536
YARN_RESOURCEMANAGER_HEAPSIZE_MB=1024
YARN_RESOURCEMANAGER_MAXIMUM_ALLOCATION_MB=1024

# Spark
SPARK_VERSION=3.3.2
SPARK_URL=https://downloads.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.2.tgz
SPARK_MASTER_HOST=spark-master
SPARK_WORKER_CORES=2
SPARK_WORKER_MEMORY=2048M
SPARK_HISTORY_SERVER_ENABLED=true

# JupyterLab
JUPYTER_ENABLE=true
JUPYTER_PASSWORD_HASH=  # Execute: python3 -c "from notebook.auth import passwd; print(passwd('sua_senha'))"
JUPYTER_PORT=8888

# Spark Connect Server (habilita modo headless, sem Jupyter)
SPARK_CONNECT_SERVER=false
SPARK_CONNECT_PORT=15002

# Versão do Java (deve ser compatível com Hadoop e Spark)
JAVA_VERSION=11

# SSH
SSH_KEY_TYPE=rsa
SSH_KEY_BITS=2048

# Portas
HOST_HDFS_UI_PORT=9870
HOST_YARN_UI_PORT=8088
HOST_SPARK_UI_PORT=7077
HOST_SPARK_HISTORY_UI_PORT=18080

# Timeout de healthchecks (segundos)
HEALTHCHECK_TIMEOUT=60


# Spark-connect server? [enable/disable]
SPARK_CONNECT_SERVER=disable
# Ao definir SPARK_CONNECT_SERVER=enable, ao subir o cluster:
# O container “master” (Spark Master) passa a expor um endpoint gRPC (na porta configurada em SPARK_CONNECT_PORT),
# para aceitar conexões de clientes externos.
# O kernel PySpark local (no próprio container ou no seu terminal) NÃO funcionará em modo interativo “local” 
# porque, ao habilitar Spark Connect, o ponto de entrada do Spark muda para esse servidor remoto.
# Consequentemente, o JupyterLab que roda no mesmo container deixará de criar um kernel PySpark “in-process” e, 
# em vez disso, qualquer notebook PySpark precisará ser configurado para se conectar via Spark Connect 
# (normalmente fornecendo o endereço gRPC do driver).

# Docker image name
IMAGE_NAME=spark:latest
STACK_NAME=spark

# Ubuntu apt mirror
# Optional: Set a custom Ubuntu APT mirror (e.g., a local or regional mirror)
# Leave this variable blank to use the default mirror (http://archive.ubuntu.com/ubuntu)
#APT_MIRROR=http://br.archive.ubuntu.com/ubuntu
APT_MIRROR=


# If you need to configure resources for Hadoop or Spark, you can directly edit the properties in the config_files/ folder. Afterwards, restart the cluster for the changes to take effect across all nodes.

##
### FILE: config_files/hadoop/mapred-site.xml:
## yarn.app.mapreduce.am.resource.memory-mb
## yarn.app.mapreduce.am.resource.vcores
## mapreduce.map.resource.memory-mb
## mapreduce.reduce.resource.memory-mb
## mapreduce.map.resource.vcores
## mapreduce.reduce.resource.vcores

##
### FILE: config_files/hadoop/yarn-site.xml:
## yarn.nodemanager.resource.memory-mb
## yarn.scheduler.maximum-allocation-mb
## yarn.scheduler.minimum-allocation-mb
## yarn.scheduler.capacity.maximum-am-resource-percent

# IMPORTANT
# mapreduce.map.resource.memory-mb <= yarn.scheduler.maximum-allocation-mb
# mapreduce.reduce.resource.memory-mb <= yarn.scheduler.maximum-allocation-mb

##
### FILE: config_files/spark/spark-defaults.conf
## spark.driver.memory
## spark.executor.memory
## spark.yarn.am.memory
## spark.executor.pyspark.memory
## spark.executor.memoryOverhead
## spark.executor.cores

# IMPORTANT
# spark.driver.memory <= yarn.scheduler.maximum-allocation-mb
# spark.executor.memory <= yarn.scheduler.maximum-allocation-mb
# spark.yarn.am.memory <= yarn.scheduler.maximum-allocation-mb
