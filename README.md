# Implanta√ß√£o de clusters Spark-Hadoop (_Cluster Deployment_) em ambiente Docker

## Introdu√ß√£o

O **Apache Hadoop** (vers√£o 3.4.1) √© um framework de c√≥digo aberto para armazenamento e processamento distribu√≠do de grandes conjuntos de dados, _Big Data_. A [**documenta√ß√£o oficial**](https://hadoop.apache.org/docs/current/) destaca a cont√≠nua evolu√ß√£o do Hadoop, com foco em melhorias de desempenho, novas funcionalidades e maior integra√ß√£o com ecossistemas de nuvem. O Hadoop √© composto por tr√™s m√≥dulos:

1. **Hadoop Distributed File System (HDFS)** √© o sistema de arquivos distribu√≠do do Hadoop, projetado para armazenar grandes volumes de dados em clusters de m√°quinas _commodity_. O HDFS oferece alta toler√¢ncia a falhas e alto _throughput_ de dados.
2. **Yet Another Resource Negotiator (YARN)** √© o framework de gerenciamento de recursos e agendamento de tarefas do Hadoop, sendo respons√°vel por alocar recursos do sistema para as v√°rias aplica√ß√µes que rodam no cluster e por agendar as tarefas a serem executadas.
3. **MapReduce** √© um modelo de programa√ß√£o e um framework de software para escrever aplica√ß√µes que processam grandes quantidades de dados em paralelo em grandes clusters.

Por sua vez, o **Apache Spark** (vers√£o 3.5.6) √© um motor de an√°lise unificado e de alta performance para processamento de dados em larga escala, de uso geral e acess√≠vel. Sua principal vantagem √© a capacidade de realizar computa√ß√£o em mem√≥ria, o que acelera significativamente o processamento em compara√ß√£o com paradigmas baseados em disco como o **MapReduce**. Al√©m disso, oferece APIs ricas e de alto n√≠vel em Java, Scala, Python e R, e um conjunto de ferramentas integradas para diversas tarefas de an√°lise de dados.

O poder do Spark reside em seus componentes bem integrados, que podem ser combinados para criar fluxos de trabalho de an√°lise complexos:
1. **Spark Core e RDDs** √© a base de toda a plataforma. 
   * **Resilient Distributed Datasets (RDDs)** √© a abstra√ß√£o fundamental do Spark, uma cole√ß√£o de elementos tolerante a falhas que pode ser operada em paralelo. Embora as APIs de alto n√≠vel como DataFrames sejam agora recomendadas, os RDDs ainda s√£o uma parte crucial da base do Spark e oferecem controle de baixo n√≠vel quando necess√°rio.
   As transforma√ß√µes em RDDs (e DataFrames) s√£o "pregui√ßosas" (_lazy evaluation_), o que significa que o Spark n√£o calcula o resultado imediatamente. Em vez disso, ele constr√≥i um grafo de execu√ß√£o (DAG) e s√≥ executa os c√°lculos quando uma a√ß√£o (como coletar os resultados) √© invocada, permitindo otimiza√ß√µes significativas.
2. **Spark SQL √© o m√≥dulo do Spark para trabalhar com dados estruturados - 'DataFrames' e 'Datasets', s√£o a principal API para manipula√ß√£o de dados. O Spark otimiza a execu√ß√£o por meio do [_Catalyst_](https://www.databricks.com/glossary/catalyst-optimizer).
   * **Motor SQL distribu√≠do** permite executar consultas SQL diretamente em DataFrames, podendo ler e escrever dados de uma variedade de fontes de dados estruturados, como JSON, Hive, Parquet e JDBC. 
   * **Adaptive Query Execution (AQE**), √© um framework que otimiza dinamicamente as consultas com base nas estat√≠sticas de tempo de execu√ß√£o, ajustando o n√∫mero de parti√ß√µes de _shuffle_ e otimizando jun√ß√µes (joins).
3. **Structured Streaming** √© o motor de processamento de stream tolerante a falhas e escal√°vel do Spark, constru√≠do sobre a API do Spark SQL.
4. **MLlib (Machine Learning Library)** √© a biblioteca de aprendizado de m√°quina do Spark, projetada para ser escal√°vel e f√°cil de usar.


## 1. Vis√£o Geral do Projeto

Bem-vindo ao reposit√≥rio **Spark-Hadoop**! Este projeto foi cuidadosamente constru√≠do para provisionar, de forma simples e r√°pida, um cluster completo que combina o poder do Apache Spark e do Apache Hadoop, via Docker, um cluster que combina o **Apache Spark** e **Apache Hadoop**, com integra√ß√£o ao **JupyterLab**. 

O objetivo √© oferecer um ambiente ‚Äú_all-in-one_‚Äù, isolado e pronto para uso, que abstrai a complexidade da configura√ß√£o manual.  √â a ferramenta interessante para estudantes, desenvolvedores e engenheiros de dados, no desenvolvimento e testes de aplica√ß√µes em Big Data, tornando simples os procedimentos de:

1. *Implantar clusters funcionais* de Spark e Hadoop em cont√™ineres Docker isolados;
2. *Aprendizado na pr√°tica*, acessando _dashboards web_ de todos os servi√ßos (HDFS, YARN, Spark UI, JupyterLab) para entender como as pe√ßas se conectam;
3. *Executar jobs de Big Data*, tanto no modelo cl√°ssico MapReduce do Hadoop quanto com o processamento em mem√≥ria de alta performance do Spark.

### **1.1. Arquitetura do Cluster**
O ambiente Docker provisiona um cluster distribu√≠do que combina:

- **Apache Hadoop 3.4.x**, fornece o HDFS (_Hadoop Distributed File System_) para armazenamento de dados massivos e tolerante a falhas, e o YARN (_Yet Another Resource Negotiator_) para um gerenciamento robusto dos recursos computacionais (CPU e mem√≥ria) do(s) cluster(s).
- **Apache Spark 4.0.x**, atua como o motor de processamento de dados em larga escala, executando tarefas sobre o YARN e lendo/escrevendo dados no HDFS.
- **JupyterLab**, oferece um ambiente de notebook interativo, pr√©-configurado com um kernel `PySpark`, permitindo a an√°lise de dados e o desenvolvimento de forma √°gil e visual.

A **arquitetura padr√£o** √© composta por:
- 1 **N√≥ Mestre** (`master`), orquestra o cluster, executando os servi√ßos de gerenciamento:
   - **HDFS NameNode**, √© "c√©rebro" do HDFS, gerencia os metadados do sistema de arquivos.
   - **YARN ResourceManager**, √© o "chefe" do YARN, aloca recursos para as aplica√ß√µes.
   - **Spark History Server**, √© uma UI web para visualizar o hist√≥rico de aplica√ß√µes Spark conclu√≠das.
   - **JupyterLab**, servidor que fornece a interface de notebooks.

- `N` **N√≥s de Trabalho** (`workers`), s√£o os "oper√°rios" do cluster que podem ser replicados dinamicamente, executando as tarefas de armazenamento e processamento:
   - **HDFS DataNode**, armazena os blocos de dados reais.  
   - **YARN NodeManager**, gerencia os recursos de uma m√°quina individual e executa as tarefas.
   - "Spark Worker", o n√∫mero de `workers` √© facilmente configur√°vel em arquivo `.env`.

---

## 2. Pr√©-requisitos
Antes de come√ßar, certifique-se dos seguintes requisitos, softwares instalados e funcionando em sua m√°quina:

- **Sistema Operacional**, preferencialmente, uma distribui√ß√£o Linux (como Ubuntu ou CentOS).
- **Java Development Kit (JDK)**, o Hadoop e o Spark rodam na JVM. √â crucial instalar uma vers√£o compat√≠vel (ex: JDK 11 para Hadoop 3.x). A vari√°vel de ambiente `JAVA_HOME` deve ser configurada e apontar para o diret√≥rio de instala√ß√£o do Java.
- **SSH (Secure Shell)**, essencial para que o n√≥ mestre possa se comunicar e gerenciar os n√≥s de trabalho sem a necessidade de senhas a cada comando. Isso √© feito gerando um par de chaves SSH (`ssh-keygen`) e copiando a chave p√∫blica para o arquivo `authorized_keys` em todos os n√≥s (inclusive no pr√≥prio mestre).
- **Docker Engine** na vers√£o 20.10.0 ou superior.
- **Docker Compose** na vers√£o V2 (`docker compose`), √© fortemente recomendada.
- **Recursos m√≠nimos** de pelo menos 8 GB de RAM alocados para o Docker, para uma experi√™ncia fluida com 2 workers.
- **Portas Livres**, verifique se as portas padr√£o (ex: 8088, 9870, 8888, 18080) n√£o est√£o em uso por outras aplica√ß√µes.

---

## 3. ‚ú®In√≠cio R√°pido
Siga estes passos para provisionar seu cluster Spark‚ÄìHadoop no ar em poucos minutos.

**3.1. Clonar o reposit√≥rio Spark-Hadoop**
Abra seu terminal e clone este reposit√≥rio para sua m√°quina local.
```
git clone https://github.com/SampMark/Spark-Hadoop.git
cd Spark-Hadoop
```
**3.2. Configurar vari√°veis de ambiente**
Crie seu arquivo `.env` a partir do template, caso necess√°rio, ajuste as vers√µes e n√∫mero de workers:

```
cp .env.template .env
# Edite .env conforme sua necessidade (HADOOP_VERSION, SPARK_VERSION, SPARK_WORKER_INSTANCES, DOWNLOAD_HADOOP_SPARK, etc.)
```

**3.3. Gerar o `docker-compose.yml` e baixar bin√°rios**

Utilize o script de inicializa√ß√£o para processar o template e (opcionalmente) baixar os _tarballs_ do Hadoop e Spark:

```
docker compose -f compose-init.yml run --rm init
```
Obserca√ß√£o: verifique as mensagens de log para garantir que docker-compose.yml foi gerado e que os arquivos foram baixados com sucesso.


**3.4. Construir as imagens Docker**
Com o Docker em execu√ß√£o, execute o seguinte comando na raiz do projeto para construir as imagens e iniciar todos os servi√ßos em _background_:
```
docker compose up
```
**3.5. Subir o cluster**
Para iniciar todos os servi√ßos em background:
```
docker compose up -d
```
   * `up`: Cria e inicia os cont√™ineres.
   * `-d`: Modo "detached" (os cont√™ineres rodam em background).

Para acompanhar logs em tempo real (ex.: master):
```
docker compose logs -f spark-master
```
O primeiro in√≠cio pode demorar alguns minutos, pois o Docker ir√° baixar as imagens base e as distribui√ß√µes do Hadoop e Spark. 
Ap√≥s a conclus√£o, seu cluster estar√° pronto para uso!

**3.6. Recriar o cluster ap√≥s altera√ß√µes**
Caso modifique o `.env` ou os templates, pare e remova os cont√™ineres antes de subir novamente:
```
docker compose down
docker compose up -d --build
```

**3.7. Acessar as UIs Web**

* HDFS NameNode: http://localhost:${HDFS_UI_PORT:-9870}
* YARN ResourceManager: http://localhost:${YARN_UI_PORT:-8088}
* Spark History Server: http://localhost:${SPARK_HISTORY_UI_PORT:-18080}
* JupyterLab: http://localhost:${JUPYTER_PORT:-8888}

---

## 4. Customiza√ß√£o do Ambiente (arquivo `.env`)
A principal forma de customizar o cluster (n√∫mero de workers, vers√µes, aloca√ß√£o de mem√≥ria, etc.) √© atrav√©s do arquivo .env. Isso evita a necessidade de editar manualmente os arquivos XML ou scripts.

Abaixo est√£o as vari√°veis mais importantes que voc√™ pode ajustar:

| Vari√°vel                      | Padr√£o (.env)   | Descri√ß√£o                                                                 |
| :---------------------------- | :-------------: | :------------------------------------------------------------------------ |
| `SPARK_WORKER_INSTANCES`      | `2`             | N√∫mero de n√≥s workers (DataNodes/NodeManagers) a serem criados no cluster |
| `HADOOP_VERSION`              | `3.4.0`         | Vers√£o do Apache Hadoop a ser utilizada                                   |
| `SPARK_VERSION`               | `3.3.4`         | Vers√£o do Apache Spark a ser utilizada                                    |
| `HDFS_REPLICATION_FACTOR`     | `2`             | Fator de replica√ß√£o padr√£o do HDFS (`dfs.replication`). Deve ser ‚â§ n√∫mero de workers |
| `YARN_NODEMANAGER_MEMORY_MB`  | `4096`          | Mem√≥ria total (MB) que cada NodeManager pode alocar para cont√™ineres (`yarn.nodemanager.resource.memory-mb`) |
| `SPARK_DRIVER_MEMORY`         | `1g`            | Mem√≥ria padr√£o para o Driver do Spark (`spark.driver.memory`)             |
| `SPARK_EXECUTOR_MEMORY`       | `1536m`         | Mem√≥ria padr√£o por Executor do Spark (`spark.executor.memory`)            |
| `SPARK_EXECUTOR_CORES`        | `2`             | N√∫mero de vCores por Executor do Spark (`spark.executor.cores`)           |
| `JUPYTERLAB_PORT`             | `8888`          | Porta local mapeada para a interface do JupyterLab                        |
| `SPARK_HISTORY_UI_PORT`       | `18080`         | Porta local mapeada para a UI do Spark History Server                     |

**Importante**: caso altere o `.env`, pode ser necess√°rio recriar os cont√™ineres para que as mudan√ßas tenham efeito: 
```
docker compose down && docker compose up -d
```
---

## 5. Acessando os Servi√ßos e UIs Web
Ap√≥s iniciar o cluster, o usu√°rio pode acessar as interfaces web dos diferentes servi√ßos atrav√©s do navegador.

| Servi√ßo              | Porta (Local) | URL de Acesso          | Descri√ß√£o |
| :------------------- | :-----------: | :--------------------: |-------------------------------------------------- |
| HDFS NameNode        | 9870          | http://localhost:9870  | UI para monitorar o estado do HDFS.               |
| YARN ResourceManager | 8088          | http://localhost:8088  | UI para monitorar o cluster, filas e aplica√ß√µes.  |
| Spark History Server | 18080         | http://localhost:18080 | UI para visualizar o hist√≥rico de aplica√ß√µes Spark. |
| JupyterLab           | 8888          | http://localhost:8888  | Ambiente interativo para notebooks PySpark.       |

---

## 6. Exemplos pr√°ticos de uso
O usu√°rio pode interagir com o cluster executando comandos dentro do cont√™iner master ou submetendo jobs.

#### 6.1. Interagindo com HDFS
Execute comandos HDFS a partir do cont√™iner `spark-master`:
```
# Listar o conte√∫do do diret√≥rio raiz do HDFS
docker compose exec spark-master hdfs dfs -ls /

# Criar um novo diret√≥rio
docker compose exec spark-master hdfs dfs -mkdir /meu-diretorio-teste

# Copiar um arquivo local (do README.md) para o HDFS
docker compose exec spark-master hdfs dfs -put README.md /meu-diretorio-teste
```
#### 6.2. Submetendo um Job Spark (SparkPi)
Execute o exemplo `SparkPi` para calcular o valor de Pi. Este job ser√° submetido ao YARN.

```
docker compose exec spark-master spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master yarn \
  --deploy-mode client \
  --num-executors 2 \
  --executor-memory 512m \
  $SPARK_HOME/examples/jars/spark-examples_*.jar 100
```
Ap√≥s a execu√ß√£o, o usu√°rio ver√° o job conclu√≠do na UI do YARN (http://localhost:8088) e na UI do Spark History Server (http://localhost:18080).

---

## 7. Gerenciando o Cluster
Use os seguintes comandos `docker compose para gerenciar o ciclo de vida do seu cluster.

* **Iniciar o cluster em background:**
```
docker compose up -d
```

* **Parar e remover os cont√™ineres:**
```
docker compose down
```

* **Verificar o status dos cont√™ineres:**
```
docker compose ps
```

* **Visualizar os logs de um servi√ßo (ex: master):**
```
docker compose logs -f spark-master
```
---

## 8. üîóRefer√™ncias e Documenta√ß√£o Oficial
Para um entendimento mais profundo dos componentes, consulte a documenta√ß√£o oficial:

* ZAHARIA, Matei _et al_. **Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing**. In: USENIX Symposium on Networked Systems Design and Implementation (NSDI), 9., 2012, San Jose, CA. Anais. Berkeley: USENIX Association, 2012. p. 1-14. Dispon√≠vel em: [https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf](https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf) Acesso em: 01 Jun. 2025.

* MCDONALD, Carol. **Introduction to Spark Processing**. In: MCDONALD, Carol. ACCELERATING APACHE SPARK 3: Leveraging NVIDIA GPUs to Power the Next Era of Analytics and Al. [S.l.]: NVIDIA, 2021. cap. 1, p. 12-30. Dispon√≠vel em: [https://images.nvidia.com/aem-dam/Solutions/deep-learning/deep-learning-ai/solutions/Accelerating-Apache-Spark-3-08262021.pdf](https://images.nvidia.com/aem-dam/Solutions/deep-learning/deep-learning-ai/solutions/Accelerating-Apache-Spark-3-08262021.pdf) Acesso em: 03 Jun. 2025.

* [Documenta√ß√£o do Apache Hadoop 3.x](https://hadoop.apache.org/docs/stable/)
* [Documenta√ß√£o do Apache Spark 3.3.x](https://spark.apache.org/docs/3.3.4/)
* [Documenta√ß√£o do Docker](https://docs.docker.com/)
* [Documenta√ß√£o do Docker Compose](https://docs.docker.com/compose/)
* [Reposit√≥rio GitHub `haddop-spark` Prof. Carlos M. D. Viegas](https://github.com/cmdviegas/hadoop-spark)

---

## üìÇ 9. Estrutura de diret√≥rios e arquivos

```
spark-hadoop/
‚îú‚îÄ‚îÄ .dockerignore
‚îú‚îÄ‚îÄ .env                            ‚Üê Arquivo principal para customiza√ß√£o do cluster.
‚îú‚îÄ‚îÄ .env.template
‚îú‚îÄ‚îÄ .gitattributes
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ .password
‚îú‚îÄ‚îÄ CHANGELOG.md
‚îú‚îÄ‚îÄ CONTRIBUTING.md
‚îú‚îÄ‚îÄ docker-compose.template.yml     ‚Üê Define os servi√ßos (master, workers) e redes do Docker.
‚îú‚îÄ‚îÄ docker-compose.yml              ‚Üê gerado por init.sh
‚îú‚îÄ‚îÄ LICENSE.apache                  ‚Üê Apache 2.0 (Hadoop)
‚îú‚îÄ‚îÄ LICENSE.mit                     ‚Üê MIT (scripts e infra)
‚îú‚îÄ‚îÄ pgsql
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ config_files/                   ‚Üê Cont√©m os TEMPLATES de configura√ß√£o.
‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ hadoop/                     ‚Üê Templates para .xml.
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ core-site.xml.template
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hadoop-env.sh.template
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hdfs-site.xml.template
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mapred-site.xml.template
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ yarn-env.sh.template
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ yarn-site.xml.template
‚îÇ   ‚îú‚îÄ‚îÄ jupyterlab/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ overrides.json.template
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ jupyter_notebook_config.py.template
‚îÇ   ‚îú‚îÄ‚îÄ spark/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ spark-defaults.conf.template
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ spark-env.sh.template
‚îÇ   ‚îî‚îÄ‚îÄ system/
‚îÇ       ‚îú‚îÄ‚îÄ README.md
‚îÇ       ‚îú‚îÄ‚îÄ bash_common
‚îÇ       ‚îî‚îÄ‚îÄ ssh_config.template
‚îú‚îÄ‚îÄ docker/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îî‚îÄ‚îÄ entrypoint.sh
‚îú‚îÄ‚îÄ myfiles/
‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ data/                    ‚Üê Coloque seus datasets aqui.
‚îÇ   ‚îú‚îÄ‚îÄ notebooks/               ‚Üê Crie seus notebooks Jupyter aqui.
‚îÇ   ‚îî‚îÄ‚îÄ scripts/                 ‚Üê Guarde seus scripts Python/Scala aqui.
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ download_all.sh
‚îÇ   ‚îú‚îÄ‚îÄ init.sh
‚îÇ   ‚îú‚îÄ‚îÄ preflight_check.sh
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap.sh             ‚Üê Script interno que cuida da formata√ß√£o do HDFS e da inicializa√ß√£o dos servi√ßos.
‚îÇ   ‚îî‚îÄ‚îÄ start_services.sh
‚îî‚îÄ‚îÄ tests/
    ‚îú‚îÄ‚îÄ smoke_test_hadoop.sh
    ‚îî‚îÄ‚îÄ smoke_test_spark.sh
```
---

## 10. Licen√ßas

- `Apache Spark` e `Apache Hadoop` s√£o software livre e de c√≥digo aberto, licenciados sob a [Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0).

- Este script √© um software livre e de c√≥digo aberto, licenciado sob [MIT License](https://opensource.org/license/mit).

