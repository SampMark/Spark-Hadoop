# docker-compose.template.yml
# ----------------------------------------------------
# ATENÇÃO: Este é um template. O arquivo final docker-compose.yml
# é gerado pelo serviço 'init'.
# ----------------------------------------------------
services:
  namenode:
    image: "${IMAGE_NAME}"
    container_name: "${STACK_NAME}_namenode"
    hostname: "namenode"
    env_file:
      - .env
    secrets:
      - user_password
    build:
      context: .
      dockerfile: ./docker/Dockerfile
      args:
        MY_USERNAME: "${MY_USERNAME}"
        HADOOP_VERSION: "${HADOOP_VERSION}"
        SPARK_VERSION: "${SPARK_VERSION}"
    environment:
        HADOOP_VERSION: "${HADOOP_VERSION}"
        SPARK_VERSION: "${SPARK_VERSION}"
        STACK_NAME: "${STACK_NAME}"
        MY_USERNAME: "${MY_USERNAME}"
        PASSWORD_FILE: "/run/secrets/user_password"
    volumes:
      - namenode_data:/home/${MY_USERNAME}/hadoop/hdfs/namenode
      - ./config_processed/hadoop:/home/${MY_USERNAME}/config/hadoop
      - ./config_processed/system:/home/${MY_USERNAME}/config/system
      - .env:/home/${MY_USERNAME}/.env:ro
      - ./myfiles:/home/${MY_USERNAME}/myfiles
    ports:
      - "${HOST_HDFS_UI_PORT}:9870"
      - "${HOST_YARN_UI_PORT}:8088"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870/"]
      interval: 10s
      timeout: 5s
      retries: 12
    command: ["bash", "/home/${MY_USERNAME}/scripts/bootstrap.sh", "master"]

  resourcemanager:
    depends_on:
      - namenode
    image: "${IMAGE_NAME}"
    container_name: "${STACK_NAME}_resourcemanager"
    hostname: "resourcemanager"
    env_file: .env
    environment:
      - MY_USERNAME=${MY_USERNAME}
    volumes:
      - ./config_processed/hadoop:/home/${MY_USERNAME}/config/hadoop
      - .env:/home/${MY_USERNAME}/.env:ro
    ports:
      - "8088:8088"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8088/"]
      interval: 10s
      timeout: 5s
      retries: 12

  spark-master:
    depends_on:
      - namenode
      - resourcemanager
    image: "${IMAGE_NAME}"
    container_name: "${STACK_NAME}_spark-master"
    hostname: "spark-master"
    env_file: .env
    environment:
      - MY_USERNAME=${MY_USERNAME}
    volumes:
      - spark_master_data:/home/${MY_USERNAME}/spark/logs
      - ./config_processed/spark:/home/${MY_USERNAME}/config/spark
      - .env:/home/${MY_USERNAME}/.env:ro
      - ./myfiles:/home/${MY_USERNAME}/myfiles
    ports:
      - "${HOST_SPARK_UI_PORT}:7077"
      - "${HOST_SPARK_HISTORY_UI_PORT}:18080"
      - "${JUPYTER_PORT}:8888"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7077/"]
      interval: 10s
      timeout: 5s
      retries: 12

  datanode:
    depends_on:
      - namenode
    image: "${IMAGE_NAME}"
    environment:
      - MY_USERNAME=${MY_USERNAME}
    deploy:
      mode: replicated
      replicas: ${NUM_WORKER_NODES}
    env_file: .env
    volumes:
      - datanode_data:/home/${MY_USERNAME}/hadoop/hdfs/datanode
      - ./config_processed/hadoop:/home/${MY_USERNAME}/config/hadoop
      - ./config_processed/system:/home/${MY_USERNAME}/config/system
      - .env:/home/${MY_USERNAME}/.env:ro
      - ./myfiles:/home/${MY_USERNAME}/myfiles
    ports:
      - "9864"  # HDFS DataNode UI
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9864/"]
      interval: 10s
      timeout: 5s
      retries: 12

  nodemanager:
    depends_on:
      - namenode
      - resourcemanager
    image: "${IMAGE_NAME}"
    environment:
      - MY_USERNAME=${MY_USERNAME}
    deploy:
      mode: replicated
      replicas: ${NUM_WORKER_NODES}
    env_file: .env
    volumes:
      - ./config_processed/hadoop:/home/${MY_USERNAME}/config/hadoop
      - .env:/home/${MY_USERNAME}/.env:ro
    healthcheck:
      test: ["CMD", "bash", "-c", "yarn node -list | grep 'RUNNING'"]
      interval: 15s
      timeout: 5s
      retries: 10

  spark-worker:
    depends_on:
      - spark-master
    image: "${STACK_NAME}_namenode_1"
    environment:
      - MY_USERNAME=${MY_USERNAME}
    deploy:
      mode: replicated
      replicas: ${NUM_WORKER_NODES}
    volumes:
      - ./config_processed/spark:/home/${MY_USERNAME}/config/spark
      - .env:/home/${MY_USERNAME}/.env:ro
      - ./myfiles:/home/${MY_USERNAME}/myfiles
    healthcheck:
      test: ["CMD", "bash", "-c", "curl -f http://spark-master:8080/"]
      interval: 15s
      timeout: 5s
      retries: 10

volumes:
  namenode_data:
  datanode_data:
  spark_master_data:

secrets:
  user_password:
    file: .password