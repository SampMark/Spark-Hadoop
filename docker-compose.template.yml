# docker-compose.template.yml
# ----------------------------------------------------
# ATENÇÃO: Este é um template. O arquivo final docker-compose.yml
# é gerado pelo serviço 'init'.
# ----------------------------------------------------
# Definição dos serviços (contêineres)
services:
  # --- SERVIÇOS HADOOP ---
  namenode:
    image: "${IMAGE_NAME}"
    container_name: "${STACK_NAME}_namenode"
    hostname: "namenode"
    env_file:
      - .env
    secrets:
      - user_password
    build:
      context: .
      dockerfile: ./docker/Dockerfile
      args:
        MY_USERNAME: "${MY_USERNAME}"
        HADOOP_VERSION: "${HADOOP_VERSION}"
        SPARK_VERSION: "${SPARK_VERSION}"
    environment:
        HADOOP_VERSION: "${HADOOP_VERSION}"
        SPARK_VERSION: "${SPARK_VERSION}"
        STACK_NAME: "${STACK_NAME}"
        MY_USERNAME: "${MY_USERNAME}"
        PASSWORD_FILE: "/run/secrets/user_password"
    volumes:
      - namenode_data:/home/${MY_USERNAME}/hadoop/hdfs/namenode
      - ./config_processed/hadoop:/home/${MY_USERNAME}/config/hadoop
      - ./config_processed/system:/home/${MY_USERNAME}/config/system
      - .env:/home/${MY_USERNAME}/.env:ro
      - ./myfiles:/home/${MY_USERNAME}/myfiles
    ports:
      - "${HOST_HDFS_UI_PORT}:9870"
      - "${HOST_YARN_UI_PORT}:8088"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870/"]
      interval: 10s
      timeout: 5s
      retries: 12
    command: ["bash", "/home/${MY_USERNAME}/scripts/bootstrap.sh", "master"]

  resourcemanager:
    depends_on:
      - namenode
    image: "${IMAGE_NAME}"
    container_name: "${STACK_NAME}_resourcemanager"
    hostname: "resourcemanager"
    env_file: .env
    environment:
      - MY_USERNAME=${MY_USERNAME}
    volumes:
      - ./config_processed/hadoop:/home/${MY_USERNAME}/config/hadoop
      - .env:/home/${MY_USERNAME}/.env:ro
    ports:
      - "8088:8088"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8088/"]
      interval: 10s
      timeout: 5s
      retries: 12
  # --- SERVIÇOS SPARK ---
  # Atenção: O serviço spark-master é o ponto de entrada para o Spark.
  # Ele deve ser iniciado após o namenode e resourcemanager.
  # Os serviços spark-worker são replicados de acordo com NUM_WORKER_NODES.
  spark-master:
    depends_on:
      - namenode
      - resourcemanager
    image: "${IMAGE_NAME}"
    container_name: "${STACK_NAME}_spark-master"
    hostname: "spark-master"
    ports:
      - "${HOST_SPARK_UI_PORT}:7077"            # Porta de submissão de jobs do Spark
      - "${HOST_SPARK_HISTORY_UI_PORT}:18080"   # Porta do UI do Spark History Server
      - "${JUPYTER_PORT}:8888"                  # Porta do Jupyter Notebook
    env_file: .env                              # Carrega variáveis de ambiente do arquivo .env
    deploy:
      mode: replicated
      replicas: 1
    environment:
      - SPARK_MODE=master
      - HADOOP_VERSION=${HADOOP_VERSION}
      - SPARK_VERSION=${SPARK_VERSION}
      - MY_USERNAME=${MY_USERNAME}
    volumes:
      - spark_master_data:/home/${MY_USERNAME}/spark/logs
      - ./config_processed/spark:/home/${MY_USERNAME}/config/spark
      - .env:/home/${MY_USERNAME}/.env:ro
      - ./myfiles:/home/${MY_USERNAME}/myfiles
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7077/"]
      interval: 10s
      timeout: 5s
      retries: 12
  
  datanode:
    depends_on:
      - namenode
    image: "${IMAGE_NAME}"
    environment:
      - MY_USERNAME=${MY_USERNAME}
    deploy:
      mode: replicated
      replicas: ${NUM_WORKER_NODES}
    env_file: .env
    volumes:
      - datanode_data:/home/${MY_USERNAME}/hadoop/hdfs/datanode
      - ./config_processed/hadoop:/home/${MY_USERNAME}/config/hadoop
      - ./config_processed/system:/home/${MY_USERNAME}/config/system
      - .env:/home/${MY_USERNAME}/.env:ro
      - ./myfiles:/home/${MY_USERNAME}/myfiles
    ports:
      - "9864"  # HDFS DataNode UI
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9864/"]
      interval: 10s
      timeout: 5s
      retries: 12

  nodemanager:
    depends_on:
      - namenode
      - resourcemanager
    image: "${IMAGE_NAME}"
    environment:
      - MY_USERNAME=${MY_USERNAME}
    deploy:
      mode: replicated
      replicas: ${NUM_WORKER_NODES}
    env_file: .env
    volumes:
      - ./config_processed/hadoop:/home/${MY_USERNAME}/config/hadoop
      - .env:/home/${MY_USERNAME}/.env:ro
    healthcheck:
      test: ["CMD", "bash", "-c", "yarn node -list | grep 'RUNNING'"]
      interval: 15s
      timeout: 5s
      retries: 10

  spark-worker:
    depends_on:
      - spark-master
    image: "${STACK_NAME}_namenode_1"
    environment:
      - MY_USERNAME=${MY_USERNAME}
    deploy:
      mode: replicated
      replicas: ${NUM_WORKER_NODES}
    volumes:
      - ./config_processed/spark:/home/${MY_USERNAME}/config/spark
      - .env:/home/${MY_USERNAME}/.env:ro
      - ./myfiles:/home/${MY_USERNAME}/myfiles
    healthcheck:
      test: ["CMD", "bash", "-c", "curl -f http://spark-master:8080/"]
      interval: 15s
      timeout: 5s
      retries: 10

# --- SERVIÇO SPARK HISTORY SERVER ---
  # O Spark History Server é responsável por manter o histórico de jobs do Spark.
  spark-history-server:
    depends_on:
      - spark-master
    image: "${IMAGE_NAME}"
    container_name: "${STACK_NAME}_spark-history-server"
    hostname: "spark-history-server"
    ports:
      - "${HOST_SPARK_HISTORY_UI_PORT}:18080"  # Porta do UI do Spark History Server
    env_file: .env
    environment:
      - SPARK_MODE=historyserver
      - HADOOP_VERSION=${HADOOP_VERSION}
      - SPARK_VERSION=${SPARK_VERSION}
      - MY_USERNAME=${MY_USERNAME}
    volumes:
      - spark_master_data:/home/${MY_USERNAME}/spark/logs
      - ./config_processed/spark:/home/${MY_USERNAME}/config/spark
      - .env:/home/${MY_USERNAME}/.env:ro
      - ./myfiles:/home/${MY_USERNAME}/myfiles
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:18080/"]
      interval: 10s
      timeout: 5s
      retries: 12

  # --- SERVIÇO JUPYTER ---
  # O Jupyter Notebook é a interface  escolhida para desenvolvimento em Python.
  jupyter:
    depends_on:
      - spark-master
    image: "${IMAGE_NAME}"
    container_name: "${STACK_NAME}_jupyter"
    hostname: "jupyter"
    ports:
      - "${JUPYTER_PORT}:8888"  # Porta do Jupyter Notebook
    env_file: .env
    environment:
      - SPARK_MODE=jupyter
      - HADOOP_VERSION=${HADOOP_VERSION}
      - SPARK_VERSION=${SPARK_VERSION}
      - MY_USERNAME=${MY_USERNAME}
    volumes:
      - spark_master_data:/home/${MY_USERNAME}/spark/logs
      - ./config_processed/spark:/home/${MY_USERNAME}/config/spark
      - .env:/home/${MY_USERNAME}/.env:ro
      - ./myfiles:/home/${MY_USERNAME}/myfiles
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8888/"]
      interval: 10s
      timeout: 5s
      retries: 12

# --- SERVIÇO INIT ---
# Este serviço é responsável por gerar o arquivo docker-compose.yml e baixar as dependências.
# Ele deve ser executado apenas uma vez para preparar o ambiente.
  init:
    build:
      context: .
      dockerfile: ./docker/Dockerfile
      target: init
    image: spark-hadoop-init:latest
    container_name: spark_hadoop_init
    volumes:
      - ./:/app:rw
    env_file:
      - .env.template
      - .env
    secrets:
      - user_password
    environment:
      - DOCKER_COMPOSE_RUN=true
      - PASSWORD_FILE=/run/secrets/user_password
    tty: true
    stdin_open: true
    command: ["bash", "/app/scripts/init.sh"]
    
volumes:
  namenode_data:
  datanode_data:
  spark_master_data:

secrets:
  user_password:
    file: .password