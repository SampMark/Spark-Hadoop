# =============================================================================
# Arquivo de Configuração de Ambiente para o Cluster Spark-Hadoop
#
# Instruções:
# 1. Copie este arquivo para .env (ex: cp .env.template .env)
# 2. Edite as variáveis conforme necessário, MY_USERNAME, NUM_WORKER_NODES, HADOOP_VERSION, SPARK_VERSION, 
# JUPYTER_PASSWORD_HASH, etc
#
# Observações:
# 1. O arquivo .env serve para configurar versões (HADOOP_VERSION, SPARK_VERSION), portas, 
# quantidade de nós, entre outros parâmetros. 
# 2. O repositório Spark-Hadoop provê o modelo `.env.template` para que o usuário possa copiar e editar, 
# documentando as opções. 
# 3. A forma de carregar essas variáveis nos contêineres utiliza a diretiva `env_file` 
# do Compose para injetar as variáveis, o que aumenta a clareza e evita expor credenciais no sistema 
# de arquivos do contêiner.
# # =============================================================================

# Usuário criado dentro do container
MY_USERNAME=sparkuser

# --- Configuração do Cluster ---
# Prefixo para nomes de contêineres e redes Docker.
STACK_NAME=spark-hadoop-cluster

# Docker image name
IMAGE_NAME=spark-hadoop-cluster:latest

# Número de instâncias de Spark Workers a serem criadas (DataNode + NodeManager + Spark Worker)
SPARK_WORKER_INSTANCES=2

# --- Versões do Software ---
# Versão do Java (deve ser compatível com Hadoop e Spark)
JAVA_VERSION=11

# --- Configurações do Hadoop ---
# Versão do Hadoop a ser utilizada
HADOOP_VERSION=3.4.1
HADOOP_URL=https://downloads.apache.org/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz

# Endereço do NameNode do HDFS (contêiner mestre)
HADOOP_NAMENODE_HOST=master
HDFS_REPLICATION_FACTOR=1
DFS_REPLICATION=1
HDFS_NAMENODE_HOST=namenode

# YARN
YARN_NODEMANAGER_RESOURCE_MEMORY_MB=2048
YARN_SCHEDULER_MAXIMUM_ALLOCATION_MB=1536
YARN_RESOURCEMANAGER_HEAPSIZE_MB=1024
YARN_RESOURCEMANAGER_MAXIMUM_ALLOCATION_MB=1024
# YARN_NM_MEM_MB=4096

# --- Configurações do Spark ---
SPARK_VERSION=3.5.6
SPARK_URL=https://downloads.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz

# Endereço do Master do Spark
SPARK_MASTER_HOST=spark-master
SPARK_WORKER_CORES=2
SPARK_WORKER_MEMORY=2048M
SPARK_HISTORY_SERVER_ENABLED=true

# Spark Connect Server (habilita modo headless, sem Jupyter)
SPARK_CONNECT_SERVER=disable
SPARK_CONNECT_PORT=15002


# --- Configuração de Rede e Portas ---
# SSH
SSH_KEY_TYPE=rsa
SSH_KEY_BITS=2048

# Portas expostas no computador (host).
HOST_HDFS_UI_PORT=9870
HOST_YARN_UI_PORT=8088
HOST_SPARK_UI_PORT=7077
HOST_SPARK_HISTORY_UI_PORT=18080
HOST_SPARK_CONNECT_PORT=15002

# Timeout de healthchecks (segundos)
HEALTHCHECK_TIMEOUT=60

# Spark-connect server? [enable/disable]
# Ao definir SPARK_CONNECT_SERVER=enable, ao subir o cluster:
# O container “master” (Spark Master) passa a expor um endpoint gRPC (na porta configurada em SPARK_CONNECT_PORT),
# para aceitar conexões de clientes externos.
# O kernel PySpark local (no próprio container ou no seu terminal) NÃO funcionará em modo interativo “local” 
# porque, ao habilitar Spark Connect, o ponto de entrada do Spark muda para esse servidor remoto.
# Consequentemente, o JupyterLab que roda no mesmo container deixará de criar um kernel PySpark “in-process” e, 
# em vez disso, qualquer notebook PySpark precisará ser configurado para se conectar via Spark Connect 
# (normalmente fornecendo o endereço gRPC do driver).

# --- Configurações do JupyterLab ---
JUPYTER_PORT=8888
JUPYTER_ENABLE=true
# Para gerar a senha, execute no seu terminal:
# python3 -c "from notebook.auth import passwd; print(passwd('sua-senha-aqui'))"
JUPYTER_PASSWORD_HASH=  # Defina aqui o hash gerado: python3 -c "from notebook.auth import passwd; print(passwd('sua_senha'))"

# Ubuntu apt mirror
# Optional: Set a custom Ubuntu APT mirror (e.g., a local or regional mirror)
# Leave this variable blank to use the default mirror (http://archive.ubuntu.com/ubuntu)
APT_MIRROR=http://br.archive.ubuntu.com/ubuntu